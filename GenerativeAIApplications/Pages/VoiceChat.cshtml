@page
@model TestWebsite.Pages.VoiceChatModel
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Chat</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
    <style>
        :root {
            --primary-color: #4f46e5;
            --primary-light: #818cf8;
            --secondary-color: #2dd4bf;
            --text-color: #1f2937;
            --bg-color: #f9fafb;
            --card-bg: #ffffff;
            --message-user: #e0f2fe;
            --message-ai: #f1f5f9;
            --border-radius: 12px;
            --shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            background-color: var(--bg-color);
            margin: 0;
            padding: 0;
            display: flex;
            justify-content: center;
            min-height: 100vh;
        }

        .app-container {
            display: flex;
            flex-direction: column;
            max-width: 800px;
            width: 100%;
            height: 100vh;
            background-color: var(--card-bg);
            box-shadow: var(--shadow);
        }

        .app-header {
            background: linear-gradient(135deg, var(--primary-color) 0%, var(--primary-light) 100%);
            color: white;
            padding: 1.5rem;
            border-radius: var(--border-radius) var(--border-radius) 0 0;
            display: flex;
            align-items: center;
            justify-content: space-between;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        .app-title {
            margin: 0;
            font-size: 1.5rem;
            font-weight: 600;
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .app-logo {
            font-size: 1.8rem;
            background: white;
            color: var(--primary-color);
            width: 40px;
            height: 40px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .chat-container {
            flex: 1;
            display: flex;
            flex-direction: column;
            overflow: hidden;
            padding: 0;
        }

        .chat-box {
            flex: 1;
            overflow-y: auto;
            padding: 1.5rem;
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }

        .message {
            padding: 12px 16px;
            border-radius: var(--border-radius);
            max-width: 80%;
            animation: fade-in 0.3s ease-out;
            position: relative;
            box-shadow: 0 1px 2px rgba(0,0,0,0.05);
        }

        @@keyframes fade-in {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }

        .user-message {
            background-color: var(--message-user);
            align-self: flex-end;
            border-bottom-right-radius: 4px;
        }

        .ai-message {
            background-color: var(--message-ai);
            align-self: flex-start;
            border-bottom-left-radius: 4px;
        }

        .message-icon {
            width: 24px;
            height: 24px;
            border-radius: 50%;
            position: absolute;
            bottom: -12px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 12px;
            color: white;
        }

        .user-message .message-icon {
            background-color: var(--primary-color);
            right: -10px;
        }

        .ai-message .message-icon {
            background-color: var(--secondary-color);
            left: -10px;
        }

        .controls-container {
            padding: 1rem;
            border-top: 1px solid rgba(0,0,0,0.05);
            background-color: #fafafa;
        }

        .status-indicator {
            display: flex;
            align-items: center;
            margin-bottom: 1rem;
            padding: 12px 16px;
            background: white;
            border-radius: var(--border-radius);
            box-shadow: 0 1px 3px rgba(0,0,0,0.05);
            transition: all 0.3s ease;
        }

        .status-circle {
            width: 12px;
            height: 12px;
            border-radius: 50%;
            margin-right: 12px;
            transition: all 0.3s ease;
        }

        .status-ready .status-circle {
            background-color: var(--secondary-color);
        }

        .status-listening .status-circle {
            background-color: #ef4444;
            animation: pulse 1.5s infinite;
        }

        .status-processing .status-circle {
            background-color: #f59e0b;
        }

        @@keyframes pulse {
            0% {
                transform: scale(0.95);
                box-shadow: 0 0 0 0 rgba(239, 68, 68, 0.7);
            }

            70% {
                transform: scale(1);
                box-shadow: 0 0 0 10px rgba(239, 68, 68, 0);
            }

            100% {
                transform: scale(0.95);
                box-shadow: 0 0 0 0 rgba(239, 68, 68, 0);
            }
        }

        .wave-animation {
            display: flex;
            align-items: center;
            gap: 3px;
            height: 20px;
            margin-left: 10px;
        }

        .wave-bar {
            width: 3px;
            height: 100%;
            background-color: var(--primary-color);
            border-radius: 3px;
            animation: wave 1s infinite ease-in-out;
        }

        .wave-bar:nth-child(2) { animation-delay: 0.1s; }
        .wave-bar:nth-child(3) { animation-delay: 0.2s; }
        .wave-bar:nth-child(4) { animation-delay: 0.3s; }
        .wave-bar:nth-child(5) { animation-delay: 0.4s; }

        @@keyframes wave {
            0%, 100% { height: 6px; }
            50% { height: 16px; }
        }

        .button-container {
            display: flex;
            gap: 12px;
            justify-content: center;
        }

        button {
            padding: 12px 24px;
            border: none;
            border-radius: var(--border-radius);
            font-weight: 600;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 8px;
            transition: all 0.2s ease;
            box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        }

        #startRecording {
            background-color: var(--primary-color);
            color: white;
            flex: 1;
            max-width: 300px;
        }

        #startRecording:hover {
            background-color: var(--primary-light);
        }

        #stopRecording {
            background-color: #f87171;
            color: white;
            flex: 1;
            max-width: 300px;
        }

        #stopRecording:hover {
            background-color: #ef4444;
        }

        .hidden {
            display: none !important;
        }

        .audio-player-container {
            padding: 0.5rem 1rem;
            border-top: 1px solid rgba(0,0,0,0.05);
            background-color: white;
        }

        #audioPlayer {
            width: 100%;
            height: 40px;
            outline: none;
        }

        @@media (max-width: 768px) {
            .app-container {
                height: 100dvh;
                width: 100%;
                border-radius: 0;
            }
            
            .app-header {
                border-radius: 0;
                padding: 1rem;
            }
            
            .message {
                max-width: 85%;
            }
        }
    </style>
</head>
<body>
    <div class="app-container">
        <header class="app-header">
            <h1 class="app-title">
                <span class="app-logo">
                    <i class="fas fa-robot"></i>
                </span>
                Voice Assistant
            </h1>
            <div class="wave-animation hidden" id="waveAnimation">
                <div class="wave-bar"></div>
                <div class="wave-bar"></div>
                <div class="wave-bar"></div>
                <div class="wave-bar"></div>
                <div class="wave-bar"></div>
            </div>
        </header>

        <div class="chat-container">
            <div class="chat-box" id="chatBox"></div>
        </div>

        <div class="audio-player-container hidden" id="audioPlayerContainer">
            <audio id="audioPlayer" controls></audio>
        </div>

        <div class="controls-container">
            <div class="status-indicator status-ready" id="statusIndicator">
                <div class="status-circle" id="statusCircle"></div>
                <span id="statusText">Ready - Just start speaking</span>
            </div>

            <div class="button-container">
                <button id="startRecording">
                    <i class="fas fa-microphone"></i>
                    Start Listening
                </button>
                <button id="stopRecording" class="hidden">
                    <i class="fas fa-stop"></i>
                    Stop
                </button>
            </div>
        </div>
    </div>

    <script>
        function applyHighPassFilter(audioContext, source) {
            const filter = audioContext.createBiquadFilter();
            filter.type = "highpass";
            filter.frequency.value = 300; // Filter out low frequencies (e.g., mic taps, background hum)
            source.connect(filter);
            return filter; // Return the filtered audio stream
        }

        function toggleRecordingUI(isRecording) {
            document.getElementById("startRecording").classList.toggle("hidden", isRecording);
            document.getElementById("stopRecording").classList.toggle("hidden", !isRecording);
            document.getElementById("statusIndicator").classList.toggle("status-listening", isRecording);
            document.getElementById("statusIndicator").classList.toggle("status-ready", !isRecording);
            document.getElementById("waveAnimation").classList.toggle("hidden", !isRecording);

            if (isRecording) {
                document.getElementById("statusText").textContent = "Listening...";
            } else {
                document.getElementById("statusText").textContent = "Ready - Just start speaking";
            }
        }

        function setupAdvancedAudioProcessing(audioContext, source) {
            const highPassFilter = audioContext.createBiquadFilter();
            highPassFilter.type = "highpass";
            highPassFilter.frequency.value = 300;
            highPassFilter.Q.value = 0.7;  // Quality factor

            const lowPassFilter = audioContext.createBiquadFilter();
            lowPassFilter.type = "lowpass";
            lowPassFilter.frequency.value = 3000;  // Human speech is generally below 3kHz
            lowPassFilter.Q.value = 0.7;

            const notchFilter = audioContext.createBiquadFilter();
            notchFilter.type = "notch";
            notchFilter.frequency.value = 60;  // Common power line frequency
            notchFilter.Q.value = 10;  // Narrow band

            const compressor = audioContext.createDynamicsCompressor();
            compressor.threshold.value = -50;
            compressor.knee.value = 40;
            compressor.ratio.value = 12;
            compressor.attack.value = 0;
            compressor.release.value = 0.25;

            source.connect(highPassFilter);
            highPassFilter.connect(notchFilter);
            notchFilter.connect(lowPassFilter);
            lowPassFilter.connect(compressor);

            return compressor;
        }

        async function calibrateAmbientNoise(analyser, frequencyData) {
            return new Promise(resolve => {
                const calibrationSamples = [];
                const CALIBRATION_DURATION = 2000; // 2 seconds
                const startTime = performance.now();

                const calibrate = () => {
                    analyser.getByteFrequencyData(frequencyData);

                    const speechEnergy = calculateSpeechEnergy(analyser, frequencyData);
                    calibrationSamples.push(speechEnergy);

                    if (performance.now() - startTime < CALIBRATION_DURATION) {
                        requestAnimationFrame(calibrate);
                    } else {
                        calibrationSamples.sort((a, b) => a - b);
                        const ambientNoiseLevel = calibrationSamples[Math.floor(calibrationSamples.length / 2)];
                        console.log("Ambient noise level calibrated:", ambientNoiseLevel);
                        resolve(ambientNoiseLevel);
                    }
                };

                calibrate();
            });
        }

        function calculateSpeechEnergy(analyser, frequencyData) {
            const sampleRate = analyser.context.sampleRate;
            const binSize = sampleRate / (analyser.frequencyBinCount * 2);
            const minBin = Math.floor(300 / binSize);  // ~300Hz
            const maxBin = Math.ceil(3000 / binSize);  // ~3000Hz

            let sum = 0;
            for (let i = minBin; i <= maxBin && i < frequencyData.length; i++) {
                sum += frequencyData[i] * frequencyData[i];
            }
            return Math.sqrt(sum / (maxBin - minBin + 1)) / 255;
        }

        function monitorForSpeechEnhanced(analyser, dataArray, frequencyData, ambientNoiseLevel, speechThresholdMultiplier = 1.0) {
            analyser.getByteTimeDomainData(dataArray);
            analyser.getByteFrequencyData(frequencyData);

            // Calculate signal energy in time domain
            let timeEnergy = calculateTimeEnergy(dataArray);

            // Calculate energy in speech frequency range (300Hz - 3000Hz)
            let speechEnergy = calculateSpeechEnergy(analyser, frequencyData);

            // Calculate energy in noise frequency ranges (below 300Hz and above 3000Hz)
            let noiseEnergy = calculateNoiseEnergy(analyser, frequencyData);

            // Calculate signal-to-noise ratio
            let snr = noiseEnergy > 0 ? speechEnergy / noiseEnergy : speechEnergy;

            // Dynamic threshold based on ambient noise level
            const dynamicThreshold = ambientNoiseLevel * 1.5 * speechThresholdMultiplier;

            // Detect speech using multiple factors
            return {
                isSpeech: speechEnergy > dynamicThreshold && snr > 1.5 && timeEnergy > 0.05,
                metrics: {
                    timeEnergy,
                    speechEnergy,
                    noiseEnergy,
                    snr,
                    threshold: dynamicThreshold
                }
            };
        }

        // Calculate time domain energy
        function calculateTimeEnergy(dataArray) {
            let sum = 0;
            for (let i = 0; i < dataArray.length; i++) {
                let amplitude = (dataArray[i] - 128) / 128;
                sum += amplitude * amplitude;
            }
            return Math.sqrt(sum / dataArray.length);
        }

        // Calculate energy in noise frequency ranges
        function calculateNoiseEnergy(analyser, frequencyData) {
            const sampleRate = analyser.context.sampleRate;
            const binSize = sampleRate / (analyser.frequencyBinCount * 2);
            const minSpeechBin = Math.floor(300 / binSize);
            const maxSpeechBin = Math.ceil(3000 / binSize);

            let sum = 0;
            let count = 0;

            // Energy below speech range
            for (let i = 0; i < minSpeechBin && i < frequencyData.length; i++) {
                sum += frequencyData[i] * frequencyData[i];
                count++;
            }

            // Energy above speech range
            for (let i = maxSpeechBin + 1; i < frequencyData.length; i++) {
                sum += frequencyData[i] * frequencyData[i];
                count++;
            }

            return count > 0 ? Math.sqrt(sum / count) / 255 : 0;
        }

        function addMessage(text, className, isInterruption = false) {
            const chatBox = document.getElementById("chatBox");
            const message = document.createElement("div");
            message.classList.add("message", className);

            // Create message icon
            const icon = document.createElement("div");
            icon.classList.add("message-icon");

            if (className === "user-message") {
                icon.innerHTML = '<i class="fas fa-user"></i>';

                // Add visual indicator for interruption if needed
                if (isInterruption) {
                    message.style.borderLeft = "3px solid #8b5cf6"; // Purple border for interruptions
                }
            } else {
                icon.innerHTML = '<i class="fas fa-robot"></i>';
            }

            message.textContent = text;
            message.appendChild(icon);
            chatBox.appendChild(message);
            chatBox.scrollTop = chatBox.scrollHeight;

            return message;
        }

        class SilenceDetector {
            constructor(analyser, ambientNoiseLevel, silenceDurationRequired = 1500) {
                this.analyser = analyser;
                this.ambientNoiseLevel = ambientNoiseLevel;
                this.SILENCE_DURATION_REQUIRED = silenceDurationRequired;
                this.silenceTimeout = null;

                // Use a rolling window for more stable silence detection
                this.WINDOW_SIZE = 30;  // 30 frames, approximately 0.5 seconds at 60fps
                this.silenceBuffer = new Array(this.WINDOW_SIZE).fill(0);
                this.bufferIndex = 0;

                // Create frequency data array
                this.frequencyData = new Uint8Array(analyser.frequencyBinCount);
            }

            update() {
                this.analyser.getByteFrequencyData(this.frequencyData);

                // Calculate energy in speech range
                const speechEnergy = calculateSpeechEnergy(this.analyser, this.frequencyData);

                // Add to rolling buffer
                this.silenceBuffer[this.bufferIndex] = speechEnergy;
                this.bufferIndex = (this.bufferIndex + 1) % this.WINDOW_SIZE;

                // Calculate average energy in buffer
                const avgEnergy = this.silenceBuffer.reduce((sum, val) => sum + val, 0) / this.WINDOW_SIZE;

                // Dynamic silence threshold based on ambient noise
                const silenceThreshold = this.ambientNoiseLevel * 1.2;

                return avgEnergy < silenceThreshold;
            }

            startMonitoring(onSilenceDetected) {
                const checkSilence = () => {
                    if (this.update()) {
                        // Silence detected
                        if (!this.silenceTimeout) {
                            this.silenceTimeout = setTimeout(() => {
                                onSilenceDetected();
                            }, this.SILENCE_DURATION_REQUIRED);
                        }
                    } else {
                        // Not silent
                        clearTimeout(this.silenceTimeout);
                        this.silenceTimeout = null;
                    }

                    this.animationFrame = requestAnimationFrame(checkSilence);
                };

                checkSilence();
            }

            stopMonitoring() {
                if (this.animationFrame) {
                    cancelAnimationFrame(this.animationFrame);
                }
                clearTimeout(this.silenceTimeout);
                this.silenceTimeout = null;
            }
        }

        class VoiceProcessor {
            constructor(automaticMode = true) {
                // Audio context and nodes
                this.audioContext = null;
                this.analyser = null;
                this.micStream = null;
                this.mediaRecorder = null;

                // State variables
                this.audioChunks = [];
                this.isRecording = false;
                this.playbackActive = false;
                this.speechDetected = false;
                this.speechStartTime = 0;
                this.ambientNoiseLevel = 0.02;  // Initial value
                this.speechThresholdMultiplier = 1.0;
                this.automaticMode = automaticMode;

                // Configuration constants - adjusted for better auto detection
                this.SPEECH_DURATION_REQUIRED = 100;  // Reduced from 300ms to 100ms for faster response
                this.SILENCE_DURATION_REQUIRED = 1500;  // 1.5 seconds of silence to stop

                // Array buffers
                this.dataArray = null;
                this.frequencyData = null;

                // Silence detector
                this.silenceDetector = null;

                // Animation frames
                this.speechMonitoringFrame = null;

                // Callbacks
                this.onRecordingStart = null;
                this.onRecordingStop = null;
                this.onSpeechDetected = null;
                this.onSilenceDetected = null;
                this.onInterruption = null;  // Added for interruption support

                // Cooldown to prevent rapid start/stop cycles
                this.lastRecordingEndTime = 0;
                this.RECORDING_COOLDOWN = 1000; // 1 second cooldown
            }

            async initialize() {
                try {
                    // Request microphone access with noise suppression
                    this.micStream = await navigator.mediaDevices.getUserMedia({
                        audio: {
                            noiseSuppression: true,
                            echoCancellation: true,
                            autoGainControl: true
                        }
                    });

                    // Set up audio context and analyser
                    this.audioContext = new AudioContext();
                    this.analyser = this.audioContext.createAnalyser();
                    this.analyser.fftSize = 2048;  // More detailed frequency analysis (increased from 1024)
                    this.analyser.smoothingTimeConstant = 0.5; // Add smoothing to make detection more stable

                    // Create arrays for time and frequency domain data
                    this.dataArray = new Uint8Array(this.analyser.fftSize);
                    this.frequencyData = new Uint8Array(this.analyser.frequencyBinCount);

                    // Create media stream source
                    const source = this.audioContext.createMediaStreamSource(this.micStream);

                    // Set up advanced audio processing chain
                    const processedSource = setupAdvancedAudioProcessing(this.audioContext, source);

                    // Connect to analyser for monitoring
                    processedSource.connect(this.analyser);

                    // Calibrate ambient noise level
                    this.ambientNoiseLevel = await calibrateAmbientNoise(this.analyser, this.frequencyData);
                    console.log("Calibrated ambient noise level:", this.ambientNoiseLevel);

                    // Initialize silence detector with adjusted parameters
                    this.silenceDetector = new SilenceDetector(this.analyser, this.ambientNoiseLevel, this.SILENCE_DURATION_REQUIRED);

                    // Start monitoring for speech (automatic mode)
                    this.startSpeechMonitoring();

                    return true;
                } catch (error) {
                    console.error("Error initializing voice processor:", error);
                    return false;
                }
            }

            startSpeechMonitoring() {
                if (this.isRecording) return; 

                // Check for cooldown period
                if (performance.now() - this.lastRecordingEndTime < this.RECORDING_COOLDOWN) {
                    // Schedule retry after cooldown period
                    setTimeout(() => this.startSpeechMonitoring(),
                        this.RECORDING_COOLDOWN - (performance.now() - this.lastRecordingEndTime));
                    return;
                }

                // Clear any existing animation frame
                if (this.speechMonitoringFrame) {
                    cancelAnimationFrame(this.speechMonitoringFrame);
                }

                const monitorSpeech = () => {
                    if (this.isRecording) return; 

                    const result = monitorForSpeechEnhanced(
                        this.analyser,
                        this.dataArray,
                        this.frequencyData,
                        this.ambientNoiseLevel,
                        this.speechThresholdMultiplier
                    );

                    // For debugging
                    if (this.automaticMode && result.metrics) {
                        // Optional debugging - uncomment if needed
                        // console.log(`Energy: ${result.metrics.speechEnergy.toFixed(4)}, Threshold: ${result.metrics.threshold.toFixed(4)}, SNR: ${result.metrics.snr.toFixed(2)}`);
                    }

                    if (result.isSpeech) {
                        if (!this.speechDetected) {
                            this.speechDetected = true;
                            this.speechStartTime = performance.now();
                            if (this.onSpeechDetected) this.onSpeechDetected();
                        } else if (this.automaticMode && (performance.now() - this.speechStartTime > this.SPEECH_DURATION_REQUIRED)) {
                            // Check if we're during playback - if so, trigger interruption
                            if (this.playbackActive) {
                                if (this.onInterruption) this.onInterruption();
                            } else {
                                this.startRecording();
                            }
                            return;
                        }
                    } else {
                        this.speechDetected = false;
                    }

                    this.speechMonitoringFrame = requestAnimationFrame(monitorSpeech);
                };

                monitorSpeech();
            }

            startRecording(isInterruption = false) {
                if (this.isRecording) return;

                // Stop speech monitoring
                if (this.speechMonitoringFrame) {
                    cancelAnimationFrame(this.speechMonitoringFrame);
                    this.speechMonitoringFrame = null;
                }

                this.mediaRecorder = new MediaRecorder(this.micStream, { mimeType: "audio/webm" });
                this.audioChunks = [];
                this.isRecording = true;

                this.mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0) {
                        this.audioChunks.push(event.data);
                    }
                };

                this.mediaRecorder.onstop = () => {
                    this.isRecording = false;
                    this.lastRecordingEndTime = performance.now();

                    if (this.onRecordingStop) {
                        const audioBlob = new Blob(this.audioChunks, { type: "audio/webm" });
                        this.onRecordingStop(audioBlob, isInterruption);
                    }

                    // Resume speech monitoring after a short delay
                    setTimeout(() => {
                        if (!this.playbackActive) {
                            this.startSpeechMonitoring();
                        }
                    }, 500);
                };

                this.mediaRecorder.start();

                // Start silence detection
                this.silenceDetector.startMonitoring(() => {
                    if (this.isRecording) {
                        this.stopRecording();
                        if (this.onSilenceDetected) this.onSilenceDetected();
                    }
                });

                if (this.onRecordingStart) this.onRecordingStart(isInterruption);
            }

            stopRecording() {
                if (!this.isRecording) return;

                // Stop silence detection
                this.silenceDetector.stopMonitoring();

                if (this.mediaRecorder && this.mediaRecorder.state !== "inactive") {
                    this.mediaRecorder.stop();
                }

                this.isRecording = false;
            }

            setPlaybackState(isPlaying) {
                this.playbackActive = isPlaying;
                // Use a lower multiplier to make interruption detection more sensitive
                this.speechThresholdMultiplier = isPlaying ? 3.0 : 1.0; // Changed from 5.0 to 3.0

                // Always maintain speech monitoring
                if (!this.speechMonitoringFrame) {
                    this.startSpeechMonitoring();
                }
            }

            // Clean up resources
            dispose() {
                this.stopRecording();

                if (this.speechMonitoringFrame) {
                    cancelAnimationFrame(this.speechMonitoringFrame);
                }

                if (this.micStream) {
                    this.micStream.getTracks().forEach(track => track.stop());
                }

                if (this.audioContext && this.audioContext.state !== "closed") {
                    this.audioContext.close();
                }
            }
        }

        // Main initialization function
        async function initializeVoiceChat() {
            const voiceProcessor = new VoiceProcessor(true); // Set to true for automatic mode
            const startButton = document.getElementById("startRecording");
            const stopButton = document.getElementById("stopRecording");
            const chatBox = document.getElementById("chatBox");
            const audioPlayer = document.getElementById("audioPlayer");
            const audioPlayerContainer = document.getElementById("audioPlayerContainer");
            const statusIndicator = document.getElementById("statusIndicator");

            // Set up callback functions
            voiceProcessor.onRecordingStart = (isInterruption) => {
                toggleRecordingUI(true);

                if (isInterruption) {
                    addMessage("Interrupting...", "user-message", true);
                } else {
                    addMessage("I'm listening...", "user-message");
                }
            };

            voiceProcessor.onRecordingStop = async (audioBlob, isInterruption) => {
                toggleRecordingUI(false);
                statusIndicator.classList.add("status-processing");
                document.getElementById("statusText").textContent = "Processing your request...";

                // Create form data for API request
                const formData = new FormData();
                formData.append("file", audioBlob, "recording.webm");

                addMessage(isInterruption ? "Processing interruption..." : "Processing your request...", "user-message", isInterruption);

                try {
                    const response = await fetch("/api/VoiceProcessor", {
                        method: "POST",
                        body: formData
                    });

                    if (response.ok) {
                        const audioBlob = await response.blob();
                        const audioUrl = URL.createObjectURL(audioBlob);

                        addMessage("AI Response Ready", "ai-message");

                        audioPlayer.src = audioUrl;
                        audioPlayerContainer.classList.remove("hidden");
                        voiceProcessor.setPlaybackState(true);

                        statusIndicator.classList.remove("status-processing");
                        document.getElementById("statusText").textContent = "Playing response...";

                        // Monitor for interruptions during playback
                        let interruptionFrames = 0; // Counter for consecutive speech frames

                        audioPlayer.onplay = () => {
                            voiceProcessor.setPlaybackState(true);

                            console.log("Starting interruption monitor");

                            // Start speech monitoring for interruptions - make this much more direct
                            let directInterruptionChecker = setInterval(() => {
                                // Skip check if already paused
                                if (audioPlayer.paused) {
                                    clearInterval(directInterruptionChecker);
                                    return;
                                }

                                // Get current speech energy directly
                                voiceProcessor.analyser.getByteFrequencyData(voiceProcessor.frequencyData);
                                const speechEnergy = calculateSpeechEnergy(
                                    voiceProcessor.analyser,
                                    voiceProcessor.frequencyData
                                );

                                // Use a simpler, more aggressive threshold for interruption
                                const threshold = voiceProcessor.ambientNoiseLevel * 2.0;

                                // Log for debugging
                                console.log(`Speech energy: ${speechEnergy}, Threshold: ${threshold}`);

                                if (speechEnergy > threshold) {
                                    console.log("INTERRUPTION DETECTED - STOPPING PLAYBACK");

                                    // Immediately clear this interval
                                    clearInterval(directInterruptionChecker);

                                    // Force stop playback
                                    audioPlayer.pause();
                                    audioPlayerContainer.classList.add("hidden");

                                    // Show interruption status
                                    document.getElementById("statusText").textContent = "Interruption detected!";

                                    // Start recording immediately
                                    setTimeout(() => {
                                        voiceProcessor.startRecording(true);
                                    }, 100);
                                }
                            }, 100); // Check every 100ms

                            // Store reference to clear on pause/end
                            audioPlayer.directInterruptionChecker = directInterruptionChecker;
                        };
                        audioPlayer.onpause = () => {
                            // Clear the interval
                            if (audioPlayer.directInterruptionChecker) {
                                clearInterval(audioPlayer.directInterruptionChecker);
                                audioPlayer.directInterruptionChecker = null;
                            }

                            voiceProcessor.setPlaybackState(false);
                            document.getElementById("statusText").textContent = "Response paused";
                        };

                        audioPlayer.onended = () => {
                            // Clear the interval
                            if (audioPlayer.directInterruptionChecker) {
                                clearInterval(audioPlayer.directInterruptionChecker);
                                audioPlayer.directInterruptionChecker = null;
                            }

                            voiceProcessor.setPlaybackState(false);
                            audioPlayerContainer.classList.add("hidden");
                            statusIndicator.classList.add("status-ready");
                            statusIndicator.classList.remove("status-processing");
                            document.getElementById("statusText").textContent = "Ready - Just start speaking";

                            // Add a slight delay before reactivating speech monitoring
                            setTimeout(() => {
                                voiceProcessor.startSpeechMonitoring();
                            }, 300);
                        };

                        audioPlayer.play();
                    } else {
                        const errorMessage = await response.text();
                        addMessage(`Error processing request: ${errorMessage}`, "ai-message");
                        statusIndicator.classList.remove("status-processing");
                        statusIndicator.classList.add("status-ready");
                        document.getElementById("statusText").textContent = "Ready - Just start speaking";
                    }
                } catch (error) {
                    console.error("Error:", error);
                    addMessage("Failed to process audio.", "ai-message");
                    statusIndicator.classList.remove("status-processing");
                    statusIndicator.classList.add("status-ready");
                    document.getElementById("statusText").textContent = "Ready - Just start speaking";
                }
            };

            voiceProcessor.onSpeechDetected = () => {
                console.log("Speech detected");
                document.getElementById("statusText").textContent = "Speech detected...";
                // Visual indicator that speech was detected
                statusIndicator.classList.add("status-listening");
                setTimeout(() => {
                    if (!voiceProcessor.isRecording) {
                        statusIndicator.classList.remove("status-listening");
                        document.getElementById("statusText").textContent = "Ready - Just start speaking";
                    }
                }, 500);
            };

            voiceProcessor.onSilenceDetected = () => {
                console.log("Silence detected");
                // The processor will automatically stop recording after silence is detected
            };

            voiceProcessor.onInterruption = () => {
                console.log("MAIN INTERRUPTION HANDLER CALLED");

                // Force stop playback immediately
                if (audioPlayer) {
                    audioPlayer.pause();
                    audioPlayerContainer.classList.add("hidden");
                }

                // Start recording with minimal delay
                setTimeout(() => {
                    voiceProcessor.startRecording(true);
                }, 50);
            };

            // Initialize the voice processor
            const initialized = await voiceProcessor.initialize();

            if (!initialized) {
                addMessage("Failed to access microphone.", "ai-message");
                return;
            }

            // Set up button event listeners (still keep these for manual override)
            startButton.addEventListener("click", () => {
                voiceProcessor.startRecording();
            });

            stopButton.addEventListener("click", () => {
                voiceProcessor.stopRecording();
            });

            // Add initial message
            if (voiceProcessor.automaticMode) {
                addMessage("Hello! I'm ready to chat. Just start speaking and I'll listen automatically.", "ai-message");
            } else {
                addMessage("Hello! Click the microphone to start speaking.", "ai-message");
            }

            // Add support for keyboard shortcuts for interruption
            document.addEventListener("keydown", (event) => {
                // Spacebar to start/stop recording
                if (event.code === "Space" && !event.target.matches("input, textarea")) {
                    event.preventDefault();

                    // If AI is speaking, interrupt it
                    if (audioPlayer && !audioPlayer.paused) {
                        audioPlayer.pause();
                        audioPlayerContainer.classList.add("hidden");
                        voiceProcessor.startRecording(true);
                    }
                    // Otherwise toggle recording state
                    else if (!startButton.classList.contains("hidden")) {
                        startButton.click();
                    } else if (!stopButton.classList.contains("hidden")) {
                        stopButton.click();
                    }
                }
            });
        }

        // Start the application
        document.addEventListener("DOMContentLoaded", initializeVoiceChat);
    </script>
</body>
</html>
